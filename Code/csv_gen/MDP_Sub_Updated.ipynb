{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use value iteration to solve\n",
    "\n",
    "using POMDPs\n",
    "using POMDPModels\n",
    "using POMDPTools\n",
    "\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Random\n",
    "\n",
    "using DiscreteValueIteration\n",
    "using Plots\n",
    "\n",
    "display(\"Begin MDP Value Iteration\")\n",
    "\n",
    "df = DataFrame(CSV.File(\"handdrawn_MDPSubterranean.csv\"))\n",
    "df_Matrix = Matrix(df)\n",
    "state_row = df_Matrix[:,3]\n",
    "state_col = df_Matrix[:,2]\n",
    "\n",
    "reward_Vec = df_Matrix[:,6]\n",
    "reward_function = Dict(GWPos(state_row[1], state_col[1]) => reward_Vec[1]) \n",
    "terminate = Dict(GWPos(5, 3) => 100)\n",
    "\n",
    "for i = 2:length(state_row)\n",
    "    reward_function[GWPos(state_row[i], state_col[i])] = reward_Vec[i]\n",
    "end    \n",
    "\n",
    "\n",
    "mdp = SimpleGridWorld(\n",
    "    size = (10,10),\n",
    "    rewards = reward_function,\n",
    "    terminate_from = Set(keys(terminate)),\n",
    "    tprob = 0.9,\n",
    "    discount = 0.9\n",
    ")\n",
    "\n",
    "solver = ValueIterationSolver(max_iterations=100, belres=1e-6, verbose=true); # creates the solver\n",
    "policy = solve(solver, mdp); # runs value iterations\n",
    "\n",
    "# policy = RandomPolicy(mdp) # Generate a random Policy\n",
    "\n",
    "render(mdp; policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the Policy at each step\n",
    "\n",
    "using POMDPs\n",
    "using POMDPModels\n",
    "using POMDPTools\n",
    "using ElectronDisplay\n",
    "ElectronDisplay.CONFIG.single_window = true\n",
    "\n",
    "ds = DisplaySimulator()\n",
    "simulate(ds, mdp, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning\n",
    "\n",
    "using DeepQLearning\n",
    "using POMDPs\n",
    "using Flux\n",
    "using POMDPModels\n",
    "using POMDPSimulators\n",
    "using POMDPTools\n",
    "\n",
    "# load MDP model from POMDPModels or define your own!\n",
    "#mdp = SimpleGridWorld();\n",
    "display(\"Begin MDP Q Learning\")\n",
    "\n",
    "# Define the Q network (see Flux.jl documentation)\n",
    "# the gridworld state is represented by a 2 dimensional vector.\n",
    "model = Chain(Dense(2, 32), Dense(32, length(actions(mdp))))\n",
    "\n",
    "exploration = EpsGreedyPolicy(mdp, LinearDecaySchedule(start=1.0, stop=0.01, steps=10000/2))\n",
    "\n",
    "solver_Q = DeepQLearningSolver(qnetwork = model, max_steps=10000, \n",
    "                             exploration_policy = exploration,\n",
    "                             learning_rate=0.005,log_freq=500,\n",
    "                             recurrence=false,double_q=true, dueling=true, prioritized_replay=true)\n",
    "policy_Q = solve(solver_Q, mdp)\n",
    "\n",
    "# render(mdp) # Could not get render to work but the rest works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9bdbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the optimal value at state s\n",
    "\n",
    "v = zeros(100)\n",
    "k = 1\n",
    "for i = 1:10\n",
    "    for j = 1:10        \n",
    "        v[k] = value(policy, [j,i]) # returns the optimal value at state s\n",
    "        k = k + 1\n",
    "    end\n",
    "end\n",
    "v = (r = v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
