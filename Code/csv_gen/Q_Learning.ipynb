{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning\n",
    "\n",
    "using CUDA\n",
    "using DeepQLearning\n",
    "using POMDPs\n",
    "using Flux\n",
    "using POMDPModels\n",
    "\n",
    "exploration = EpsGreedyPolicy(mdp, LinearDecaySchedule(start=1.0, stop=0.01, steps=10000/2))\n",
    "\n",
    "# the model weights will be send to the gpu in the call to solve\n",
    "model_Q2 = Chain(Dense(2, 32), Dense(32, length(actions(mdp))))\n",
    "\n",
    "solver_Q2 = DeepQLearningSolver(qnetwork = model, max_steps=100, exploration_policy = exploration,\n",
    "                             learning_rate=0.005,log_freq=500,\n",
    "                             recurrence=false,double_q=true, dueling=true, prioritized_replay=true)\n",
    "policy_Q2 = solve(solver, mdp)\n",
    "\n",
    "sim = RolloutSimulator(max_steps=100000)\n",
    "r_tot = simulate(sim, mdp, policy_Q2)\n",
    "println(\"Total discounted reward for 1 simulation: $r_tot\")\n",
    "\n",
    "render(mdp; policy = policy_Q2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
